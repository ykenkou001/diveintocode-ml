{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。  \n",
    "\n",
    "（例）  \n",
    "\n",
    "・重みを初期化する必要があった  \n",
    "・エポックのループが必要だった  \n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解答\n",
    "・ミニバッチ処理をする必要があった。  \n",
    "・フォーワードプロパゲーション    \n",
    "・活性化関数の使用  \n",
    "・交差エントロピー誤差（損失関数）（損失の算出）  \n",
    "・バックプロパゲーション  \n",
    "・最適化処理(勾配降下法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。  \n",
    "\n",
    "Iris Species  \n",
    "\n",
    "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。  \n",
    "\n",
    "Iris-versicolor  \n",
    "Iris-virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。  \n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解答\n",
    "・ミニバッチ処理をする必要があった。=> 一緒。  \n",
    "・フォーワードプロパゲーション =>　example_net関数の中で定義されている。　  \n",
    "・活性化関数の使用  => tf.nn.relu関数を使用し、example_net関数の中で定義されている。  \n",
    "・交差エントロピー誤差（損失関数）（損失の算出） => tf.reduce_mean関数を使用し実装している。    \n",
    "・バックプロパゲーション  =>   \n",
    "・最適化処理(勾配降下法）=> tf.train.AdamOptimizer()関数とoptimizer.minimize()関数を使用し定義されている。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.他のデータセットへの適用\n",
    "これまで扱ってきた小さなデータセットがいくつかあります。下記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。  \n",
    "\n",
    "・Iris（3種類全ての目的変数を使用）  \n",
    "・House Prices  \n",
    "\n",
    "どのデータセットもtrain, val, testの3種類に分けて使用してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     30
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0909 21:39:36.152631 15948 deprecation.py:323] From C:\\Users\\ykenk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 25.3703, val_loss : 59.9979, acc : 0.750, val_acc : 0.375\n",
      "Epoch 1, loss : 34.3246, val_loss : 22.8902, acc : 0.250, val_acc : 0.625\n",
      "Epoch 2, loss : 3.1277, val_loss : 10.7089, acc : 0.750, val_acc : 0.375\n",
      "Epoch 3, loss : 0.0000, val_loss : 1.1875, acc : 1.000, val_acc : 0.812\n",
      "Epoch 4, loss : 3.4757, val_loss : 6.8689, acc : 0.750, val_acc : 0.688\n",
      "Epoch 5, loss : 0.0000, val_loss : 0.8853, acc : 1.000, val_acc : 0.875\n",
      "Epoch 6, loss : 0.0000, val_loss : 1.6928, acc : 1.000, val_acc : 0.875\n",
      "Epoch 7, loss : 0.0000, val_loss : 0.9750, acc : 1.000, val_acc : 0.875\n",
      "Epoch 8, loss : 0.0000, val_loss : 0.5479, acc : 1.000, val_acc : 0.875\n",
      "Epoch 9, loss : 0.8967, val_loss : 4.7755, acc : 0.750, val_acc : 0.750\n",
      "test_acc : 0.800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y) #numpy配列化\n",
    "X = np.array(X) #numpy配列化\n",
    "\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "#print(y)\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "#print(\"X.shape\",X.shape)\n",
    "#print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1] #入力データの特徴量\n",
    "n_samples = X_train.shape[0] #サンプル数\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # forward?\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1']) # 線形結合\n",
    "    layer_1 = tf.nn.relu(layer_1) # relu関数\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']) # 線形結合\n",
    "    layer_2 = tf.nn.relu(layer_2) # relu関数\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法 \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss #なぜ+=？\n",
    "            total_acc += acc\n",
    "            #print(\"total_loss1\",total_loss)\n",
    "        total_loss /= n_samples\n",
    "        #print(\"total_loss2\", total_loss)\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。  \n",
    "\n",
    "Iris Species  \n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u = df['state'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     36
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_one_hot [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "X_test [[5.8 2.8 5.1 2.4]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.1 2.8 4.  1.3]]\n",
      "n_input 4\n",
      "loss_op Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "Epoch 0, loss : 2.7518, val_loss : 2.8234, acc : 0.667, val_acc : 0.708\n",
      "Epoch 1, loss : 2.1351, val_loss : 14.1748, acc : 0.833, val_acc : 0.625\n",
      "Epoch 2, loss : 1.2618, val_loss : 14.4071, acc : 0.833, val_acc : 0.625\n",
      "Epoch 3, loss : 0.0000, val_loss : 1.1777, acc : 1.000, val_acc : 0.833\n",
      "Epoch 4, loss : 0.4088, val_loss : 3.0105, acc : 0.833, val_acc : 0.875\n",
      "Epoch 5, loss : 0.8740, val_loss : 3.7439, acc : 0.833, val_acc : 0.833\n",
      "Epoch 6, loss : 0.0000, val_loss : 1.3120, acc : 1.000, val_acc : 0.917\n",
      "Epoch 7, loss : 0.0000, val_loss : 1.5982, acc : 1.000, val_acc : 0.917\n",
      "Epoch 8, loss : 0.0000, val_loss : 1.5218, acc : 1.000, val_acc : 0.917\n",
      "Epoch 9, loss : 0.0000, val_loss : 2.2666, acc : 1.000, val_acc : 0.917\n",
      "test_acc : 0.933\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "#df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "\n",
    "\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "y = np.array(y) #numpy配列化\n",
    "X = np.array(X) #numpy配列化\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(\"y_test_one_hot\",y_test_one_hot[:10])\n",
    "print('X_test', X_test[:10])\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1] #入力データの特徴量\n",
    "print(\"n_input\",n_input)\n",
    "n_samples = X_train.shape[0] #サンプル数\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # forward?\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1']) # 線形結合\n",
    "    layer_1 = tf.nn.relu(layer_1) # relu関数\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']) # 線形結合\n",
    "    layer_2 = tf.nn.relu(layer_2) # relu関数\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "#tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "# 目的関数---\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "print(\"loss_op\",loss_op)\n",
    "# 最適化手法 \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果----\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            #print(\"total_loss\",total_loss)\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710</td>\n",
       "      <td>2003</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>1976</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786</td>\n",
       "      <td>2001</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1717</td>\n",
       "      <td>1915</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2198</td>\n",
       "      <td>2000</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GrLivArea  YearBuilt  SalePrice\n",
       "0       1710       2003     208500\n",
       "1       1262       1976     181500\n",
       "2       1786       2001     223500\n",
       "3       1717       1915     140000\n",
       "4       2198       2000     250000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1460, step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_selected = df.loc[:, [\"GrLivArea\", \"YearBuilt\", \"SalePrice\"]]\n",
    "display(df_selected.head())\n",
    "display(df_selected.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     48
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 32.61024475097656, val_loss : 32.533\n",
      "Epoch 1, loss : 21.976518630981445, val_loss : 11.016\n",
      "Epoch 2, loss : 15.289836883544922, val_loss : 6.601\n",
      "Epoch 3, loss : 10.573897361755371, val_loss : 4.743\n",
      "Epoch 4, loss : 7.517651557922363, val_loss : 3.733\n",
      "Epoch 5, loss : 5.438677787780762, val_loss : 3.120\n",
      "Epoch 6, loss : 3.6929659843444824, val_loss : 2.685\n",
      "Epoch 7, loss : 2.5599045753479004, val_loss : 2.395\n",
      "Epoch 8, loss : 1.8665069341659546, val_loss : 2.185\n",
      "Epoch 9, loss : 1.3928158283233643, val_loss : 2.027\n",
      "test_mse : 2.374\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df_selected = df.loc[:, [\"GrLivArea\", \"YearBuilt\", \"SalePrice\"]]\n",
    "df_selected.head()\n",
    "#df_conversion = df['SalePrice'].apply(np.log)\n",
    "\n",
    "# データフレームから条件抽出\n",
    "X = df[['GrLivArea', 'YearBuilt']] # 2つの特徴量を抜き出し変数に格納\n",
    "y_conversion = df[['SalePrice']].apply(np.log) # 目的変数を抜き出し、変数に格納\n",
    "\n",
    "y_conversion = np.array(y_conversion) #numpy配列化\n",
    "X = np.array(X) #numpy配列化\n",
    "\n",
    "# print(\"X\", X[:10])\n",
    "# print(\"y\", y[:10])\n",
    "# ラベルを数値に変換\n",
    "\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_conversion, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "# print(\"X.shape\",X.shape)\n",
    "# print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "\n",
    "#標準化処理を行う\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#　標準化する\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# 標準化変形する\n",
    "\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1] #入力データの特徴量\n",
    "n_samples = X_train.shape[0] #サンプル数\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train_transformed, y_conversion, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # forward?\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1']) # 線形結合\n",
    "    layer_1 = tf.nn.relu(layer_1) # relu関数\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']) # 線形結合\n",
    "    layer_2 = tf.nn.relu(layer_2) # relu関数\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 指標値計算(コスト関数)\n",
    "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
    "\n",
    "# 最適化手法 \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#誤差の記録\n",
    "total_loss_0 = []\n",
    "total_val_loss_0 = []\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = []\n",
    "        total_val_loss = []\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            #print(\"loss\",type(loss))\n",
    "        total_loss_0.append(np.exp(loss)) #指数変換\n",
    "            \n",
    "            \n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val_transformed, Y: y_val})\n",
    "        total_val_loss_0.append(np.exp(val_loss)) #指数変換\n",
    "        print(\"Epoch {}, loss : {:}, val_loss : {:.3f}\".format(epoch, loss, val_loss))\n",
    "    test_mse = sess.run(loss_op, feed_dict={X: X_test_transformed, Y: y_test})\n",
    "    print(\"test_mse : {:.3f}\".format(test_mse))\n",
    "#print(\"total_loss_0\",total_loss_0)\n",
    "#print(\"total_val_loss_0\",total_val_loss_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(total_loss_0))\n",
    "\n",
    "plt.title(\"Decrease of Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(x, total_loss_0, label = 'loss')\n",
    "plt.plot(x, total_val_loss_0, label = 'val loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "y_train_one_hot.shape (48000, 10)\n",
      "Epoch 0, loss : 0.7559, val_loss : 1.0966, acc : 0.600, val_acc : 0.660\n",
      "Epoch 1, loss : 0.7274, val_loss : 0.9384, acc : 0.800, val_acc : 0.720\n",
      "Epoch 2, loss : 0.4415, val_loss : 0.6229, acc : 0.900, val_acc : 0.793\n",
      "Epoch 3, loss : 0.5286, val_loss : 0.4100, acc : 0.800, val_acc : 0.893\n",
      "Epoch 4, loss : 0.3888, val_loss : 0.3927, acc : 0.900, val_acc : 0.906\n",
      "Epoch 5, loss : 0.4120, val_loss : 0.4090, acc : 0.900, val_acc : 0.912\n",
      "Epoch 6, loss : 0.5007, val_loss : 0.3357, acc : 0.900, val_acc : 0.917\n",
      "Epoch 7, loss : 0.5132, val_loss : 0.3662, acc : 0.900, val_acc : 0.912\n",
      "Epoch 8, loss : 0.5936, val_loss : 0.3012, acc : 0.900, val_acc : 0.928\n",
      "Epoch 9, loss : 0.0841, val_loss : 0.3859, acc : 1.000, val_acc : 0.919\n",
      "test_acc : 0.916\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# print(X_train.shape) # (60000, 28, 28)\n",
    "# print(X_test.shape) # (10000, 28, 28)\n",
    "# print(X_train[0].dtype) # uint8\n",
    "# #print(X_train[0])\n",
    "# データフレームから条件抽出\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "#trainデータを2次元に変換\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_val = X_val.reshape(-1, 784)\n",
    "\n",
    "#０～１のfloat型に変換(正規化)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_val = X_val.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "print(X_train.shape)\n",
    "#one-hot-encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(\"y_train_one_hot.shape\",y_train_one_hot.shape)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
    "\n",
    "# 指標値計算\n",
    "#y_train_one_hot.shape\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#誤差の記録\n",
    "total_loss_0 = []\n",
    "total_val_loss_0 = []\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "90px",
    "width": "265px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245.76px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
