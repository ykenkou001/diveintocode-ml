{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T11:16:02.621253Z",
     "start_time": "2019-10-30T11:16:02.615226Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T11:15:36.111285Z",
     "start_time": "2019-10-30T11:15:36.105298Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#import graphviz\n",
    "\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\ykenk\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\ykenk\\anaconda3\\lib\\site-packages (from pydot) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】公式Exampleを分担して実行\n",
    "TensorFLowの公式Exampleを分担して実行してください。\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://colab.research.google.com/drive/1QuiyxydNXiKRsWt4PGVndsM-1XsEz_hn#scrollTo=cDq0CIKc1vO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 【問題2】Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (100, 4)\n",
      "X_train.shape (64, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y) #numpy配列化\n",
    "X = np.array(X) #numpy配列化\n",
    "\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "#print(y)\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(\"X.shape\",X.shape)\n",
    "print(\"X_train.shape\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentialモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0909 15:58:15.804632 13688 deprecation.py:506] From C:\\Users\\ykenk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Sequentialモデル\n",
    "#ロジスティック回帰を作るために、全結合層のクラス、tf.keras.layers.Denseを使います。\n",
    "#引数に出力のユニット数、活性化関数、入力のユニット数を入れます。\n",
    "\n",
    "#モデルを定義する\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, activation = tf.nn.sigmoid, input_shape=(4,))])\n",
    "\n",
    "#作成したモデルの構造を見る\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 15:58:15.859485 13688 deprecation.py:323] From C:\\Users\\ykenk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.9532 - acc: 0.6406\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 739us/sample - loss: 0.4790 - acc: 0.8906\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 671us/sample - loss: 0.4813 - acc: 0.8750\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 649us/sample - loss: 0.4792 - acc: 0.9062\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 618us/sample - loss: 0.4546 - acc: 0.9062\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 637us/sample - loss: 0.4606 - acc: 0.9219\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 641us/sample - loss: 0.4468 - acc: 0.9062\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 603us/sample - loss: 0.4261 - acc: 0.9219\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.4288 - acc: 0.9062\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 635us/sample - loss: 0.4136 - acc: 0.9375\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.4081 - acc: 0.8906\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 614us/sample - loss: 0.4018 - acc: 0.9688\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 632us/sample - loss: 0.4027 - acc: 0.9219\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 617us/sample - loss: 0.3940 - acc: 0.9531\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 624us/sample - loss: 0.3763 - acc: 0.9375\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 642us/sample - loss: 0.3824 - acc: 0.9219\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 718us/sample - loss: 0.3776 - acc: 0.9375\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 654us/sample - loss: 0.3738 - acc: 0.9531\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 701us/sample - loss: 0.3528 - acc: 0.9375\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 669us/sample - loss: 0.3498 - acc: 0.9531\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.3531 - acc: 0.9219\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 670us/sample - loss: 0.3404 - acc: 0.9531\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.3334 - acc: 0.9531\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 653us/sample - loss: 0.3246 - acc: 0.9688\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.3271 - acc: 0.9375\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 592us/sample - loss: 0.3167 - acc: 0.9531\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 604us/sample - loss: 0.3113 - acc: 0.9531\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 621us/sample - loss: 0.3151 - acc: 0.9375\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 649us/sample - loss: 0.3149 - acc: 0.9531\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 635us/sample - loss: 0.2999 - acc: 0.9531\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 647us/sample - loss: 0.2965 - acc: 0.9531\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 629us/sample - loss: 0.2931 - acc: 0.9688\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 618us/sample - loss: 0.3107 - acc: 0.9375\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 654us/sample - loss: 0.2930 - acc: 0.9531\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 635us/sample - loss: 0.2918 - acc: 0.9531\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 673us/sample - loss: 0.2859 - acc: 0.9375\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 686us/sample - loss: 0.2772 - acc: 0.9531\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 630us/sample - loss: 0.2667 - acc: 0.9688\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 619us/sample - loss: 0.2689 - acc: 0.9531\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 615us/sample - loss: 0.2670 - acc: 0.9688\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 646us/sample - loss: 0.2629 - acc: 0.9531\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.2619 - acc: 0.9688\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 699us/sample - loss: 0.2614 - acc: 0.9531\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 561us/sample - loss: 0.2645 - acc: 0.9688\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 717us/sample - loss: 0.2577 - acc: 0.9844\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 766us/sample - loss: 0.2601 - acc: 0.9375\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 624us/sample - loss: 0.2456 - acc: 0.9688\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 631us/sample - loss: 0.2602 - acc: 0.9375\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 610us/sample - loss: 0.2381 - acc: 0.9375\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 649us/sample - loss: 0.2475 - acc: 0.9531\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.2311 - acc: 0.9844\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 612us/sample - loss: 0.2354 - acc: 0.9531\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 625us/sample - loss: 0.2331 - acc: 0.9375\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 629us/sample - loss: 0.2229 - acc: 0.9688\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.2313 - acc: 0.9531\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 620us/sample - loss: 0.2305 - acc: 0.9531\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 600us/sample - loss: 0.2300 - acc: 0.9531\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 591us/sample - loss: 0.2210 - acc: 0.9688\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 609us/sample - loss: 0.2208 - acc: 0.9531\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 632us/sample - loss: 0.2116 - acc: 0.9688\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 614us/sample - loss: 0.2281 - acc: 0.9375\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 624us/sample - loss: 0.2140 - acc: 0.9531\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 628us/sample - loss: 0.2221 - acc: 0.9688\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 673us/sample - loss: 0.2087 - acc: 0.9688\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 679us/sample - loss: 0.2061 - acc: 0.9844\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 618us/sample - loss: 0.2034 - acc: 0.9375\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 670us/sample - loss: 0.2074 - acc: 0.9531\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 655us/sample - loss: 0.2014 - acc: 0.9531\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.2019 - acc: 0.9688\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.2010 - acc: 0.9531\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 622us/sample - loss: 0.2006 - acc: 0.9531\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 616us/sample - loss: 0.1955 - acc: 0.9844\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 611us/sample - loss: 0.2015 - acc: 0.9688\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 624us/sample - loss: 0.1898 - acc: 0.9688\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1988 - acc: 0.9375\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 631us/sample - loss: 0.1887 - acc: 0.9688\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 664us/sample - loss: 0.1864 - acc: 0.9688\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1870 - acc: 0.9844\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 617us/sample - loss: 0.1897 - acc: 0.9688\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 627us/sample - loss: 0.1879 - acc: 0.9531\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 617us/sample - loss: 0.1819 - acc: 0.9688\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.1928 - acc: 0.9844\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 636us/sample - loss: 0.1859 - acc: 0.9688\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 692us/sample - loss: 0.1775 - acc: 0.9688\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.1907 - acc: 0.9688\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 617us/sample - loss: 0.1743 - acc: 0.9844\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 601us/sample - loss: 0.1771 - acc: 0.9844\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 616us/sample - loss: 0.1740 - acc: 0.9531\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 638us/sample - loss: 0.1713 - acc: 0.9688\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.1710 - acc: 0.9688\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 590us/sample - loss: 0.1672 - acc: 0.9531\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 638us/sample - loss: 0.1828 - acc: 0.9531\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 626us/sample - loss: 0.1711 - acc: 0.9531\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 632us/sample - loss: 0.1688 - acc: 0.9688\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 612us/sample - loss: 0.1725 - acc: 0.9531\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.1653 - acc: 0.9688\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1642 - acc: 0.9688\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 682us/sample - loss: 0.1590 - acc: 0.9688\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.1625 - acc: 0.9531\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1622 - acc: 0.9688\n"
     ]
    }
   ],
   "source": [
    "# モデルをコンパイルする。\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習する\n",
    "history = model.fit(X_train, y_train,batch_size=1,epochs=100,verbose=1)\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1611 - acc: 0.9688 - val_loss: 0.1273 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1685 - acc: 0.9844 - val_loss: 0.1361 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 761us/sample - loss: 0.1552 - acc: 0.9844 - val_loss: 0.1490 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 795us/sample - loss: 0.1594 - acc: 0.9531 - val_loss: 0.1248 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 795us/sample - loss: 0.1555 - acc: 0.9688 - val_loss: 0.1334 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 766us/sample - loss: 0.1565 - acc: 0.9531 - val_loss: 0.1226 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 752us/sample - loss: 0.1639 - acc: 0.9688 - val_loss: 0.1232 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 756us/sample - loss: 0.1632 - acc: 0.9688 - val_loss: 0.1235 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 750us/sample - loss: 0.1561 - acc: 0.9531 - val_loss: 0.1206 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1511 - acc: 0.9688 - val_loss: 0.1213 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1533 - acc: 0.9688 - val_loss: 0.1308 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 766us/sample - loss: 0.1566 - acc: 0.9688 - val_loss: 0.1431 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 765us/sample - loss: 0.1583 - acc: 0.9531 - val_loss: 0.1282 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1521 - acc: 0.9844 - val_loss: 0.1251 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 764us/sample - loss: 0.1528 - acc: 0.9688 - val_loss: 0.1268 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 755us/sample - loss: 0.1491 - acc: 0.9844 - val_loss: 0.1177 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 767us/sample - loss: 0.1482 - acc: 0.9688 - val_loss: 0.1258 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 773us/sample - loss: 0.1467 - acc: 0.9844 - val_loss: 0.1164 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 750us/sample - loss: 0.1501 - acc: 0.9688 - val_loss: 0.1157 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 781us/sample - loss: 0.1549 - acc: 0.9531 - val_loss: 0.1324 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 787us/sample - loss: 0.1466 - acc: 0.9531 - val_loss: 0.1103 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 793us/sample - loss: 0.1431 - acc: 0.9844 - val_loss: 0.1289 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 777us/sample - loss: 0.1442 - acc: 0.9844 - val_loss: 0.1145 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 770us/sample - loss: 0.1426 - acc: 0.9531 - val_loss: 0.1109 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 761us/sample - loss: 0.1482 - acc: 0.9844 - val_loss: 0.1075 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 752us/sample - loss: 0.1417 - acc: 0.9688 - val_loss: 0.1071 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 765us/sample - loss: 0.1452 - acc: 0.9688 - val_loss: 0.1140 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 765us/sample - loss: 0.1441 - acc: 0.9688 - val_loss: 0.1127 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1394 - acc: 0.9531 - val_loss: 0.1050 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 779us/sample - loss: 0.1352 - acc: 0.9844 - val_loss: 0.1292 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1448 - acc: 0.9375 - val_loss: 0.1066 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 779us/sample - loss: 0.1393 - acc: 0.9688 - val_loss: 0.1200 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 839us/sample - loss: 0.1369 - acc: 0.9688 - val_loss: 0.1142 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 792us/sample - loss: 0.1492 - acc: 0.9688 - val_loss: 0.1125 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 784us/sample - loss: 0.1400 - acc: 0.9688 - val_loss: 0.1082 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 749us/sample - loss: 0.1349 - acc: 0.9844 - val_loss: 0.1129 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1398 - acc: 0.9531 - val_loss: 0.1039 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1390 - acc: 0.9844 - val_loss: 0.1082 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 810us/sample - loss: 0.1491 - acc: 0.9531 - val_loss: 0.1056 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1382 - acc: 0.9531 - val_loss: 0.1144 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 757us/sample - loss: 0.1344 - acc: 0.9844 - val_loss: 0.0977 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 776us/sample - loss: 0.1418 - acc: 0.9531 - val_loss: 0.0984 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1358 - acc: 0.9688 - val_loss: 0.1099 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 816us/sample - loss: 0.1335 - acc: 0.9688 - val_loss: 0.1007 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 741us/sample - loss: 0.1343 - acc: 0.9688 - val_loss: 0.0984 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 749us/sample - loss: 0.1287 - acc: 0.9688 - val_loss: 0.1099 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 746us/sample - loss: 0.1365 - acc: 0.9688 - val_loss: 0.1023 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 762us/sample - loss: 0.1325 - acc: 0.9688 - val_loss: 0.0946 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 857us/sample - loss: 0.1313 - acc: 0.9844 - val_loss: 0.0937 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 873us/sample - loss: 0.1333 - acc: 0.9688 - val_loss: 0.0974 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 920us/sample - loss: 0.1282 - acc: 0.9688 - val_loss: 0.1003 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 798us/sample - loss: 0.1340 - acc: 0.9688 - val_loss: 0.0965 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 822us/sample - loss: 0.1257 - acc: 0.9688 - val_loss: 0.0921 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 778us/sample - loss: 0.1237 - acc: 0.9844 - val_loss: 0.0950 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 829us/sample - loss: 0.1310 - acc: 0.9531 - val_loss: 0.0931 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 789us/sample - loss: 0.1383 - acc: 0.9531 - val_loss: 0.1000 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 769us/sample - loss: 0.1258 - acc: 0.9688 - val_loss: 0.0913 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 735us/sample - loss: 0.1298 - acc: 0.9531 - val_loss: 0.0941 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1302 - acc: 0.9688 - val_loss: 0.0915 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 789us/sample - loss: 0.1265 - acc: 0.9688 - val_loss: 0.0898 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 751us/sample - loss: 0.1269 - acc: 0.9688 - val_loss: 0.0888 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 766us/sample - loss: 0.1241 - acc: 0.9688 - val_loss: 0.0899 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 761us/sample - loss: 0.1371 - acc: 0.9688 - val_loss: 0.1077 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1345 - acc: 0.9688 - val_loss: 0.1024 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 761us/sample - loss: 0.1270 - acc: 0.9688 - val_loss: 0.0899 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 741us/sample - loss: 0.1240 - acc: 0.9688 - val_loss: 0.0899 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 772us/sample - loss: 0.1208 - acc: 0.9688 - val_loss: 0.0860 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1256 - acc: 0.9688 - val_loss: 0.0863 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 772us/sample - loss: 0.1195 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 756us/sample - loss: 0.1246 - acc: 0.9688 - val_loss: 0.0929 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 769us/sample - loss: 0.1191 - acc: 0.9688 - val_loss: 0.0888 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 765us/sample - loss: 0.1227 - acc: 0.9688 - val_loss: 0.0874 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 767us/sample - loss: 0.1174 - acc: 0.9688 - val_loss: 0.0941 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1225 - acc: 0.9688 - val_loss: 0.1016 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 771us/sample - loss: 0.1183 - acc: 0.9688 - val_loss: 0.0949 - val_acc: 0.9375\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 759us/sample - loss: 0.1252 - acc: 0.9688 - val_loss: 0.0897 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 745us/sample - loss: 0.1176 - acc: 0.9844 - val_loss: 0.0842 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 768us/sample - loss: 0.1228 - acc: 0.9688 - val_loss: 0.0838 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 752us/sample - loss: 0.1217 - acc: 0.9531 - val_loss: 0.0862 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 765us/sample - loss: 0.1178 - acc: 0.9531 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 767us/sample - loss: 0.1184 - acc: 0.9688 - val_loss: 0.0884 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 786us/sample - loss: 0.1216 - acc: 0.9531 - val_loss: 0.0808 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 744us/sample - loss: 0.1215 - acc: 0.9688 - val_loss: 0.0918 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 776us/sample - loss: 0.1213 - acc: 0.9688 - val_loss: 0.0825 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 765us/sample - loss: 0.1207 - acc: 0.9688 - val_loss: 0.0865 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 748us/sample - loss: 0.1129 - acc: 0.9688 - val_loss: 0.0803 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 739us/sample - loss: 0.1177 - acc: 0.9688 - val_loss: 0.0795 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 759us/sample - loss: 0.1227 - acc: 0.9531 - val_loss: 0.0797 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 801us/sample - loss: 0.1169 - acc: 0.9688 - val_loss: 0.0787 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 795us/sample - loss: 0.1354 - acc: 0.9531 - val_loss: 0.0786 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 759us/sample - loss: 0.1192 - acc: 0.9688 - val_loss: 0.0788 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 774us/sample - loss: 0.1147 - acc: 0.9531 - val_loss: 0.0784 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 763us/sample - loss: 0.1107 - acc: 0.9688 - val_loss: 0.0782 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 764us/sample - loss: 0.1087 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 779us/sample - loss: 0.1208 - acc: 0.9531 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 810us/sample - loss: 0.1076 - acc: 0.9688 - val_loss: 0.0824 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 781us/sample - loss: 0.1122 - acc: 0.9688 - val_loss: 0.0770 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 792us/sample - loss: 0.1094 - acc: 0.9688 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 777us/sample - loss: 0.1160 - acc: 0.9844 - val_loss: 0.0757 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 752us/sample - loss: 0.1100 - acc: 0.9688 - val_loss: 0.0772 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#検証用データ有り\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_proba [0.9784603  0.0226194  0.9054172  0.00861695 0.9265773  0.09216115\n",
      " 0.00887373 0.08864832 0.9322368  0.9582959  0.01551679 0.82399607\n",
      " 0.09862658 0.6060616  0.6091738  0.01736388 0.35191756 0.9170152\n",
      " 0.02445331 0.9839746  0.9964077  0.09379575 0.79680735 0.9942898\n",
      " 0.9863446  0.1437943  0.9696576  0.01127425 0.9576485  0.98137623\n",
      " 0.98450685 0.9711     0.15709287 0.92624414 0.12212679 0.99954075\n",
      " 0.99299484 0.0074645  0.02019256 0.3992207  0.00164464 0.73707587\n",
      " 0.55114144 0.00138071 0.03135842 0.10728243 0.99726814 0.99991584\n",
      " 0.00911981 0.06472316 0.01118314 0.98780787 0.9812871  0.9982866\n",
      " 0.9952344  0.9193299  0.05271339 0.9691935  0.74206394 0.98555994\n",
      " 0.01838729 0.09411049 0.95131046 0.75807846]\n",
      "y_pred [1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1\n",
      " 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 推定\n",
    "y_pred_proba = model.predict(X_train)[:, 0]\n",
    "\n",
    "# 確率を0, 1に変換\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.10537443682551384\n",
      "Train acccuracy 0.96875\n"
     ]
    }
   ],
   "source": [
    "# 結果なし、評価のみ\n",
    "score= model.evaluate(X_train, y_train ,verbose=0)\n",
    "print(\"Train loss\", score[0])\n",
    "print(\"Train acccuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0909 15:58:25.676979 13688 deprecation_wrapper.py:119] From C:\\Users\\ykenk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0909 15:58:25.677978 13688 deprecation_wrapper.py:119] From C:\\Users\\ykenk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0909 15:58:25.679971 13688 deprecation_wrapper.py:119] From C:\\Users\\ykenk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 2ms/sample - loss: 0.7015 - acc: 0.6719\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 681us/sample - loss: 0.5227 - acc: 0.8281\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 627us/sample - loss: 0.5480 - acc: 0.6406\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 627us/sample - loss: 0.5223 - acc: 0.7344\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 656us/sample - loss: 0.5012 - acc: 0.8281\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.5022 - acc: 0.7500\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 697us/sample - loss: 0.4820 - acc: 0.8438\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 685us/sample - loss: 0.4693 - acc: 0.8438\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 810us/sample - loss: 0.4629 - acc: 0.8906\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.4547 - acc: 0.8906\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.4488 - acc: 0.8125\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 633us/sample - loss: 0.4483 - acc: 0.8281\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 627us/sample - loss: 0.4292 - acc: 0.8750\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 627us/sample - loss: 0.4125 - acc: 0.9062\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 611us/sample - loss: 0.4047 - acc: 0.8906\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 617us/sample - loss: 0.4072 - acc: 0.9062\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 655us/sample - loss: 0.3938 - acc: 0.9219\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 620us/sample - loss: 0.4060 - acc: 0.8750\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 624us/sample - loss: 0.4064 - acc: 0.8750\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 614us/sample - loss: 0.3754 - acc: 0.9219\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 726us/sample - loss: 0.3733 - acc: 0.9219\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 654us/sample - loss: 0.3613 - acc: 0.9531\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 637us/sample - loss: 0.3618 - acc: 0.9219\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 643us/sample - loss: 0.3483 - acc: 0.9531\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.3495 - acc: 0.9375\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 607us/sample - loss: 0.3521 - acc: 0.9531\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 625us/sample - loss: 0.3416 - acc: 0.9375\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 625us/sample - loss: 0.3181 - acc: 0.9531\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 625us/sample - loss: 0.3405 - acc: 0.9375\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 629us/sample - loss: 0.3263 - acc: 0.9375\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.3235 - acc: 0.9375\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 622us/sample - loss: 0.3116 - acc: 0.9531\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 629us/sample - loss: 0.3096 - acc: 0.9531\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 628us/sample - loss: 0.3144 - acc: 0.9375\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 626us/sample - loss: 0.3031 - acc: 0.9531\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 626us/sample - loss: 0.2948 - acc: 0.9531\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 636us/sample - loss: 0.2934 - acc: 0.9531\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 635us/sample - loss: 0.2903 - acc: 0.9531\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 634us/sample - loss: 0.3081 - acc: 0.9219\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 624us/sample - loss: 0.2838 - acc: 0.9375\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 685us/sample - loss: 0.2830 - acc: 0.9375\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 857us/sample - loss: 0.2743 - acc: 0.9688\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 888us/sample - loss: 0.2902 - acc: 0.9219\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 966us/sample - loss: 0.2752 - acc: 0.9375\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 686us/sample - loss: 0.2783 - acc: 0.9531\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 670us/sample - loss: 0.2715 - acc: 0.9531\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 711us/sample - loss: 0.2597 - acc: 0.9531\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 656us/sample - loss: 0.2683 - acc: 0.9688\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 561us/sample - loss: 0.2496 - acc: 0.9531\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 588us/sample - loss: 0.2634 - acc: 0.9375\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 596us/sample - loss: 0.2499 - acc: 0.9844\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.2598 - acc: 0.9531\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 607us/sample - loss: 0.2483 - acc: 0.9531\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 595us/sample - loss: 0.2387 - acc: 0.9531\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 592us/sample - loss: 0.2480 - acc: 0.9375\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 577us/sample - loss: 0.2469 - acc: 0.9531\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.2469 - acc: 0.9531\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 594us/sample - loss: 0.2327 - acc: 0.9531\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 577us/sample - loss: 0.2400 - acc: 0.9062\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 561us/sample - loss: 0.2279 - acc: 0.9531\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 564us/sample - loss: 0.2287 - acc: 0.9688\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 569us/sample - loss: 0.2242 - acc: 0.9531\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 576us/sample - loss: 0.2226 - acc: 0.9688\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 586us/sample - loss: 0.2319 - acc: 0.9531\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 575us/sample - loss: 0.2374 - acc: 0.9531\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 611us/sample - loss: 0.2141 - acc: 0.9688\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 629us/sample - loss: 0.2142 - acc: 0.9688\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 620us/sample - loss: 0.2170 - acc: 0.9688\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 659us/sample - loss: 0.2155 - acc: 0.9375\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 717us/sample - loss: 0.2129 - acc: 0.9844\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 701us/sample - loss: 0.2078 - acc: 0.9688\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 749us/sample - loss: 0.2045 - acc: 0.9688\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.2053 - acc: 0.9531\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 717us/sample - loss: 0.2012 - acc: 0.9688\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.2114 - acc: 0.9531\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 764us/sample - loss: 0.1978 - acc: 0.9531\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 717us/sample - loss: 0.2001 - acc: 0.9688\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 670us/sample - loss: 0.2055 - acc: 0.9375\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1991 - acc: 0.9688\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.1914 - acc: 0.9688\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 592us/sample - loss: 0.2015 - acc: 0.9688\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1932 - acc: 0.9531\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 592us/sample - loss: 0.1937 - acc: 0.9531\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 604us/sample - loss: 0.1903 - acc: 0.9531\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 608us/sample - loss: 0.1878 - acc: 0.9688\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 655us/sample - loss: 0.2088 - acc: 0.9531\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1841 - acc: 0.9688\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 611us/sample - loss: 0.1888 - acc: 0.9688\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 685us/sample - loss: 0.1790 - acc: 0.9688\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1858 - acc: 0.9844\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 836us/sample - loss: 0.1788 - acc: 0.9531\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 780us/sample - loss: 0.1873 - acc: 0.9688\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1801 - acc: 0.9219\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 795us/sample - loss: 0.1768 - acc: 0.9688\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 670us/sample - loss: 0.1748 - acc: 0.9688\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 623us/sample - loss: 0.1828 - acc: 0.9375\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1755 - acc: 0.9688\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 826us/sample - loss: 0.1721 - acc: 0.9844\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 639us/sample - loss: 0.1751 - acc: 0.9531\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.1802 - acc: 0.9688\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# 入力層\n",
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "# 出力層\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(input_data)\n",
    "#モデルを定義する\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "#以下、Sequntialモデルと一緒\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=100,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.16249998658895493\n",
      "Train accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(96, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "#df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "display(y.unique())\n",
    "\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "# display(X.head())\n",
    "y = np.array(y) #numpy配列化\n",
    "X = np.array(X) #numpy配列化\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "#Functional\n",
    "#3層NN\n",
    "\n",
    "#one-hot-encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "#input~output~インスタンス渡し\n",
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#多値分類\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_one_hot,\n",
    "                    batch_size=1,\n",
    "                    epochs=100,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11973557621240616\n",
      "Train accuracy: 0.9583333\n"
     ]
    }
   ],
   "source": [
    "# 推定\n",
    "score = model.evaluate(X_train, y_train_one_hot, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 2)\n",
      "(234, 2)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df_selected = df.loc[:, [\"GrLivArea\", \"YearBuilt\", \"SalePrice\"]]\n",
    "df_selected.head()\n",
    "#df_conversion = df['SalePrice'].apply(np.log)\n",
    "\n",
    "# データフレームから条件抽出\n",
    "X = df[['GrLivArea', 'YearBuilt']] # 2つの特徴量を抜き出し変数に格納\n",
    "y_conversion = df[['SalePrice']].apply(np.log) # 目的変数を抜き出し、変数に格納\n",
    "\n",
    "y_conversion = np.array(y_conversion) #numpy配列化\n",
    "X = np.array(X) #numpy配列化\n",
    "\n",
    "# print(\"X\", X[:10])\n",
    "# print(\"y\", y[:10])\n",
    "# ラベルを数値に変換\n",
    "\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_conversion, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "# print(\"X.shape\",X.shape)\n",
    "# print(\"X_train.shape\", X_train.shape)\n",
    "\n",
    "\n",
    "#標準化処理を行う\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#　標準化する\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# 標準化変形する\n",
    "\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "print(X_train_transformed.shape)\n",
    "print(X_val_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(100, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(100, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(100, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,601\n",
      "Trainable params: 20,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/20\n",
      "934/934 [==============================] - 0s 162us/sample - loss: 452.0144 - val_loss: 79.4410\n",
      "Epoch 2/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 40.4175 - val_loss: 14.4420\n",
      "Epoch 3/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 9.4792 - val_loss: 3.3809\n",
      "Epoch 4/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 1.7601 - val_loss: 1.1524\n",
      "Epoch 5/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.8240 - val_loss: 0.6342\n",
      "Epoch 6/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 0.4308 - val_loss: 0.2917\n",
      "Epoch 7/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 0.2462 - val_loss: 0.1286\n",
      "Epoch 8/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.1448 - val_loss: 0.1595\n",
      "Epoch 9/20\n",
      "934/934 [==============================] - 0s 15us/sample - loss: 0.1366 - val_loss: 0.1251\n",
      "Epoch 10/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.1096 - val_loss: 0.0893\n",
      "Epoch 11/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 0.0963 - val_loss: 0.1297\n",
      "Epoch 12/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.1144 - val_loss: 0.0846\n",
      "Epoch 13/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.0877 - val_loss: 0.1002\n",
      "Epoch 14/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 0.0929 - val_loss: 0.1173\n",
      "Epoch 15/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.0825 - val_loss: 0.1048\n",
      "Epoch 16/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 0.0693 - val_loss: 0.0541\n",
      "Epoch 17/20\n",
      "934/934 [==============================] - 0s 13us/sample - loss: 0.0753 - val_loss: 0.0763\n",
      "Epoch 18/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.0551 - val_loss: 0.0569\n",
      "Epoch 19/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.0544 - val_loss: 0.0833\n",
      "Epoch 20/20\n",
      "934/934 [==============================] - 0s 14us/sample - loss: 0.0789 - val_loss: 0.0910\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.1),\n",
    "              )\n",
    "history = model.fit(X_train_transformed, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                   validation_data=(X_val_transformed, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1644.13170386]\n",
      " [1896.06321125]\n",
      " [1828.65593246]\n",
      " [1831.45425912]\n",
      " [1798.34154861]\n",
      " [1987.07674077]\n",
      " [1970.99062059]\n",
      " [1587.00449543]\n",
      " [1970.84248508]\n",
      " [2031.95709698]\n",
      " [1593.12244208]\n",
      " [1655.28459811]\n",
      " [1666.00333559]\n",
      " [1616.91050254]\n",
      " [2239.81638802]\n",
      " [1894.34256608]\n",
      " [1641.14020318]\n",
      " [1692.42554647]\n",
      " [1735.89612596]\n",
      " [1826.25539365]\n",
      " [1841.05882606]\n",
      " [1578.07133728]\n",
      " [1754.01770897]\n",
      " [1878.0995446 ]\n",
      " [1710.07188815]\n",
      " [1736.72464459]\n",
      " [1678.6237698 ]\n",
      " [1740.67670667]\n",
      " [1685.30989475]\n",
      " [2132.82631257]\n",
      " [1524.53219088]\n",
      " [1741.75896308]\n",
      " [1784.40646265]\n",
      " [1672.67993423]\n",
      " [3102.13740197]\n",
      " [1645.5326325 ]\n",
      " [1802.29766186]\n",
      " [1749.39439519]\n",
      " [1816.03292811]\n",
      " [1663.40006612]\n",
      " [1964.73823921]\n",
      " [1680.73447813]\n",
      " [1890.46116368]\n",
      " [1583.8861788 ]\n",
      " [2003.8730297 ]\n",
      " [1836.75099559]\n",
      " [1547.67892522]\n",
      " [1716.81970764]\n",
      " [1849.91465391]\n",
      " [1966.83862242]\n",
      " [1724.29611011]\n",
      " [1704.35311757]\n",
      " [1874.99096586]\n",
      " [1593.5466413 ]\n",
      " [1696.12817767]\n",
      " [1831.65819781]\n",
      " [1667.13930912]\n",
      " [1847.71729787]\n",
      " [1695.5478191 ]\n",
      " [1593.08134041]\n",
      " [1593.3227488 ]\n",
      " [1748.56749636]\n",
      " [1865.99022864]\n",
      " [1796.15245545]\n",
      " [1862.30000599]\n",
      " [1869.61154078]\n",
      " [2081.44947022]\n",
      " [2189.4460139 ]\n",
      " [1838.81189739]\n",
      " [1845.13448337]\n",
      " [1660.03367278]\n",
      " [1576.35575263]\n",
      " [1557.10857246]\n",
      " [1915.20636808]\n",
      " [1720.87034141]\n",
      " [1748.28359651]\n",
      " [1798.96509368]\n",
      " [1998.3223296 ]\n",
      " [1795.61225771]\n",
      " [1590.5624905 ]\n",
      " [1645.39409358]\n",
      " [1734.87444484]\n",
      " [1771.61037059]\n",
      " [1606.5473671 ]\n",
      " [1703.00796647]\n",
      " [1918.32889544]\n",
      " [1970.52341433]\n",
      " [1783.50791937]\n",
      " [1898.57242291]\n",
      " [1889.21714271]\n",
      " [1899.85050508]\n",
      " [1735.48855809]\n",
      " [1578.30572192]\n",
      " [1601.68890354]\n",
      " [1743.56212148]\n",
      " [1894.5617464 ]\n",
      " [1888.68949365]\n",
      " [2061.08629715]\n",
      " [1882.21967032]\n",
      " [1536.49291375]\n",
      " [2255.40764898]\n",
      " [1736.94136798]\n",
      " [1608.20930783]\n",
      " [1447.0812568 ]\n",
      " [1894.56223468]\n",
      " [1742.24446674]\n",
      " [2553.4672672 ]\n",
      " [1844.78768161]\n",
      " [1647.21374256]\n",
      " [1952.53734397]\n",
      " [2041.62291107]\n",
      " [1654.80144287]\n",
      " [1809.29361165]\n",
      " [1858.83041877]\n",
      " [1900.53247651]\n",
      " [1922.00715384]\n",
      " [1887.24773278]\n",
      " [1955.23339338]\n",
      " [1878.92735645]\n",
      " [1860.26795311]\n",
      " [1826.53836338]\n",
      " [1704.25043519]\n",
      " [1891.51440368]\n",
      " [1760.21710454]\n",
      " [1846.13068169]\n",
      " [1743.5855383 ]\n",
      " [1576.47174714]\n",
      " [1705.18419238]\n",
      " [1767.47658789]\n",
      " [1847.20549341]\n",
      " [1923.53026102]\n",
      " [1871.18722366]\n",
      " [1744.7825716 ]\n",
      " [1870.51285069]\n",
      " [1834.04823404]\n",
      " [1922.77452013]\n",
      " [2021.45353887]\n",
      " [1900.20182123]\n",
      " [1715.74503141]\n",
      " [1997.36995367]\n",
      " [1542.51048934]\n",
      " [1787.87052655]\n",
      " [1549.20258974]\n",
      " [1835.92873849]\n",
      " [2017.83588249]\n",
      " [1847.22625466]\n",
      " [2007.0281793 ]\n",
      " [1664.03922431]\n",
      " [1791.32847617]\n",
      " [1761.30468338]\n",
      " [1904.00902635]\n",
      " [1882.23880306]\n",
      " [1847.11894061]\n",
      " [1721.58899897]\n",
      " [1636.98534253]\n",
      " [1592.47814431]\n",
      " [1933.01622557]\n",
      " [1766.30196949]\n",
      " [1705.48268   ]\n",
      " [1678.27191233]\n",
      " [1802.84157274]\n",
      " [1934.03219647]\n",
      " [1757.11402864]\n",
      " [1660.43237787]\n",
      " [1783.4030771 ]\n",
      " [2262.20714578]\n",
      " [1380.79233375]\n",
      " [1807.32227045]\n",
      " [2148.3041863 ]\n",
      " [1878.70752005]\n",
      " [1772.30410404]\n",
      " [1626.79160484]\n",
      " [1732.6691631 ]\n",
      " [1533.02347822]\n",
      " [1753.17760104]\n",
      " [1786.63373345]\n",
      " [1842.72514161]\n",
      " [2002.21087309]\n",
      " [1639.54495299]\n",
      " [1677.24533493]\n",
      " [1992.24859379]\n",
      " [1642.34832255]\n",
      " [1909.6593577 ]\n",
      " [1897.82562038]\n",
      " [1815.70680711]\n",
      " [1852.63618432]\n",
      " [1874.28490272]\n",
      " [1551.19337531]\n",
      " [1762.96887763]\n",
      " [1766.54343584]\n",
      " [1873.73815132]\n",
      " [1609.41443947]\n",
      " [1582.93710061]\n",
      " [2195.29744202]\n",
      " [1946.56242341]\n",
      " [1825.61934206]\n",
      " [1885.22381374]\n",
      " [1711.05098361]\n",
      " [1714.14265956]\n",
      " [1642.52476036]\n",
      " [1705.2099542 ]\n",
      " [1765.92914808]\n",
      " [1660.33229863]\n",
      " [1783.26170379]\n",
      " [1763.96361365]\n",
      " [1966.38319438]\n",
      " [1983.59361165]\n",
      " [1935.39293494]\n",
      " [1820.30357487]\n",
      " [1588.89158039]\n",
      " [1688.73108548]\n",
      " [1542.7097414 ]\n",
      " [1691.88231805]\n",
      " [1834.9191398 ]\n",
      " [1742.64443404]\n",
      " [2089.31970762]\n",
      " [1826.32532286]\n",
      " [1670.84120149]\n",
      " [1943.12337412]\n",
      " [1656.02749001]\n",
      " [1692.50885081]\n",
      " [1791.74963935]\n",
      " [2005.7808048 ]\n",
      " [1657.31840104]\n",
      " [1451.92611812]\n",
      " [1739.85325255]\n",
      " [1844.22778876]\n",
      " [1664.45669602]\n",
      " [1610.77401624]\n",
      " [1952.30484506]\n",
      " [1700.74512498]\n",
      " [1969.65713712]\n",
      " [1895.1502758 ]\n",
      " [1876.59098693]\n",
      " [1912.82307834]\n",
      " [1645.31311379]\n",
      " [1787.97846326]\n",
      " [1968.06251689]\n",
      " [1825.08171352]\n",
      " [1821.2589819 ]\n",
      " [1576.26635717]\n",
      " [1885.06748625]\n",
      " [1692.43401916]\n",
      " [1867.6473548 ]\n",
      " [1843.37547656]\n",
      " [1796.70215424]\n",
      " [1856.26768837]\n",
      " [1808.43143568]\n",
      " [1570.66376177]\n",
      " [1712.0932305 ]\n",
      " [1709.84838294]\n",
      " [1881.38326364]\n",
      " [1969.21180933]\n",
      " [1620.4731602 ]\n",
      " [1590.51049294]\n",
      " [1732.39468114]\n",
      " [2014.78179299]\n",
      " [1965.27118966]\n",
      " [1798.31887096]\n",
      " [1733.88365472]\n",
      " [2074.90130431]\n",
      " [1812.3065813 ]\n",
      " [1962.08576459]\n",
      " [1860.7532586 ]\n",
      " [1981.32354327]\n",
      " [1707.94209143]\n",
      " [1859.44042734]\n",
      " [1946.43645044]\n",
      " [1963.08897107]\n",
      " [1718.29161648]\n",
      " [1908.88340462]\n",
      " [1670.19322263]\n",
      " [1909.08587346]\n",
      " [1824.28566865]\n",
      " [1575.59884351]\n",
      " [1626.3782381 ]\n",
      " [1771.32676458]\n",
      " [1675.29967978]\n",
      " [1853.38753261]\n",
      " [1811.42426936]\n",
      " [1840.84149144]\n",
      " [1567.06196369]\n",
      " [1763.66956538]\n",
      " [1737.1557871 ]\n",
      " [2666.55051195]\n",
      " [1909.36571414]\n",
      " [2177.23468907]\n",
      " [1773.36881612]\n",
      " [1989.30271331]\n",
      " [1873.79811067]\n",
      " [1766.09394454]\n",
      " [2076.00106756]\n",
      " [1990.79422623]\n",
      " [1649.013053  ]\n",
      " [1558.60423934]\n",
      " [1589.33277934]\n",
      " [1782.45648362]\n",
      " [1756.45707679]\n",
      " [1790.35505043]\n",
      " [1843.60604149]\n",
      " [1803.31726849]\n",
      " [1907.0940641 ]\n",
      " [1960.10257036]\n",
      " [1611.7408556 ]\n",
      " [1849.81780754]\n",
      " [1856.2804758 ]\n",
      " [1747.11624349]\n",
      " [1736.83016675]\n",
      " [1966.3754713 ]\n",
      " [1744.29027908]\n",
      " [1937.86453803]\n",
      " [1920.5825397 ]\n",
      " [1907.18613997]\n",
      " [1737.52579031]\n",
      " [1963.95971267]\n",
      " [1738.98055921]\n",
      " [1897.88386118]\n",
      " [1740.89835192]\n",
      " [1856.95998094]\n",
      " [1587.16187922]\n",
      " [2435.81142702]\n",
      " [1511.75283508]\n",
      " [1841.57143622]\n",
      " [1749.16353244]\n",
      " [1902.10201602]\n",
      " [1809.7400696 ]\n",
      " [1924.80160594]\n",
      " [1987.65408627]\n",
      " [1754.47226116]\n",
      " [1713.36487397]\n",
      " [1912.84370197]\n",
      " [1492.36544149]\n",
      " [1639.78285485]\n",
      " [1720.56944629]\n",
      " [1963.95447322]\n",
      " [1714.31293169]\n",
      " [1603.97646208]\n",
      " [2093.45929844]\n",
      " [1917.60132714]\n",
      " [1898.73029969]\n",
      " [1696.84684346]\n",
      " [1891.21826719]\n",
      " [1728.58353532]\n",
      " [1842.5107029 ]\n",
      " [1576.13260908]\n",
      " [1541.26982361]\n",
      " [2167.45163402]\n",
      " [1505.76017492]\n",
      " [1696.37425575]\n",
      " [1993.11557908]\n",
      " [1892.24547991]\n",
      " [1686.03044181]\n",
      " [1807.17670822]\n",
      " [2198.21039735]\n",
      " [1673.94922919]\n",
      " [1763.32896707]\n",
      " [1711.4727892 ]\n",
      " [1736.22311715]\n",
      " [1917.92708871]\n",
      " [1901.35526711]\n",
      " [1855.74156961]\n",
      " [1894.50614522]\n",
      " [1938.082592  ]\n",
      " [1854.43671591]\n",
      " [2597.69286702]\n",
      " [1877.05562127]\n",
      " [1803.38963973]\n",
      " [1843.66577087]\n",
      " [1903.83098812]\n",
      " [1613.12253219]\n",
      " [1750.34883257]\n",
      " [1630.29813728]\n",
      " [1987.70086296]\n",
      " [1974.60848842]\n",
      " [1807.05957811]\n",
      " [1590.05949951]\n",
      " [1780.01909965]\n",
      " [1853.95631282]\n",
      " [1568.15798154]\n",
      " [1625.38943119]\n",
      " [1623.43063766]\n",
      " [1867.22030569]\n",
      " [1621.07813169]\n",
      " [2157.75364513]\n",
      " [1581.53443187]\n",
      " [1797.21139429]\n",
      " [1890.32261401]\n",
      " [1795.61259804]\n",
      " [1967.43799507]\n",
      " [1684.31619359]\n",
      " [1959.42132863]\n",
      " [1933.50137524]\n",
      " [1997.30955235]\n",
      " [1737.42733655]\n",
      " [1567.08781021]\n",
      " [1869.98648532]\n",
      " [1769.19047355]\n",
      " [1826.37909925]\n",
      " [1819.41625403]\n",
      " [1718.19090945]\n",
      " [1514.11008303]\n",
      " [1657.06112476]\n",
      " [1801.05313571]\n",
      " [1946.61524315]\n",
      " [1622.81886559]\n",
      " [1819.73409274]\n",
      " [1622.4962292 ]\n",
      " [1862.90904724]\n",
      " [1859.13582505]\n",
      " [1965.97624536]\n",
      " [1917.49899613]\n",
      " [1824.33069428]\n",
      " [2025.7354754 ]\n",
      " [1734.24651123]\n",
      " [1910.53215661]\n",
      " [1766.58267406]\n",
      " [1870.06069009]\n",
      " [1499.76943607]\n",
      " [1939.76543035]\n",
      " [1555.52874861]\n",
      " [1738.03733715]\n",
      " [1895.23287795]\n",
      " [1721.67246516]\n",
      " [1837.1812912 ]\n",
      " [1740.78371396]\n",
      " [1821.77695768]\n",
      " [1872.73260669]\n",
      " [1723.58878434]\n",
      " [1678.69778213]\n",
      " [1807.04705979]\n",
      " [1919.18130456]\n",
      " [1653.39610964]\n",
      " [1940.65902639]\n",
      " [1693.26938278]\n",
      " [1575.12007541]\n",
      " [1625.0289143 ]\n",
      " [1768.04494726]\n",
      " [1872.40834484]\n",
      " [1655.85987727]\n",
      " [1721.86685375]\n",
      " [1862.94084557]\n",
      " [1842.25888797]\n",
      " [1572.31196067]\n",
      " [1631.44699515]\n",
      " [1806.53550352]\n",
      " [1675.0333214 ]\n",
      " [1739.15058903]\n",
      " [1764.06570713]\n",
      " [1935.43645732]\n",
      " [1862.72945407]\n",
      " [1961.27926081]\n",
      " [1612.21706357]\n",
      " [1865.91682139]\n",
      " [1894.80272498]\n",
      " [1982.56069415]\n",
      " [1855.3975477 ]\n",
      " [1638.88481914]\n",
      " [1641.31006409]\n",
      " [1659.35421558]\n",
      " [1698.8666907 ]\n",
      " [1957.49920223]\n",
      " [1722.97385704]\n",
      " [1677.85173678]\n",
      " [2253.29082285]\n",
      " [1740.40095994]\n",
      " [1841.50896683]\n",
      " [1821.41141583]\n",
      " [1621.37746396]\n",
      " [1642.01332245]\n",
      " [1920.49114099]\n",
      " [1765.19809865]\n",
      " [1567.16491166]\n",
      " [1897.65010378]\n",
      " [1622.29354692]\n",
      " [1686.31378969]\n",
      " [1720.01727522]\n",
      " [1577.66875903]\n",
      " [1773.26950926]\n",
      " [1795.64972069]\n",
      " [1681.75751177]\n",
      " [1822.55938948]\n",
      " [1800.2921481 ]\n",
      " [1964.10783716]\n",
      " [1891.97326732]\n",
      " [1885.81873918]\n",
      " [1967.21592611]\n",
      " [1721.69664793]\n",
      " [2710.78022549]\n",
      " [1873.99836621]\n",
      " [1608.86617933]\n",
      " [1895.830596  ]\n",
      " [1825.81320936]\n",
      " [1972.31608388]\n",
      " [1898.35034736]\n",
      " [1871.2149541 ]\n",
      " [1992.39283586]\n",
      " [1870.49677224]\n",
      " [1855.44466501]\n",
      " [1823.26478442]\n",
      " [1677.64833507]\n",
      " [1854.50214613]\n",
      " [1810.43531036]\n",
      " [1716.6582366 ]\n",
      " [1783.10990841]\n",
      " [1972.28819103]\n",
      " [1992.38343485]\n",
      " [1870.23214625]\n",
      " [2120.07964836]\n",
      " [1879.82735194]\n",
      " [1636.34706445]\n",
      " [1675.26898264]\n",
      " [1834.70661192]\n",
      " [1640.4474908 ]\n",
      " [1919.29191407]\n",
      " [2096.01419817]\n",
      " [1592.0092999 ]\n",
      " [1645.97052073]\n",
      " [1873.89610779]\n",
      " [1692.80345828]\n",
      " [2004.98477668]\n",
      " [1640.83976989]\n",
      " [1995.62648809]\n",
      " [1793.14952761]\n",
      " [1564.85694404]\n",
      " [1798.78080094]\n",
      " [1845.39704286]\n",
      " [1913.56314986]\n",
      " [1971.4318567 ]\n",
      " [1843.22617006]\n",
      " [1532.5807292 ]\n",
      " [1656.3183176 ]\n",
      " [1817.06326439]\n",
      " [1779.64317508]\n",
      " [1980.11991212]\n",
      " [1959.44848311]\n",
      " [1742.05253194]\n",
      " [1645.32638616]\n",
      " [1718.01825964]\n",
      " [1745.22949768]\n",
      " [1781.43260593]\n",
      " [1893.24347093]\n",
      " [1835.06201581]\n",
      " [1565.8603205 ]\n",
      " [1652.65628775]\n",
      " [1944.31566771]\n",
      " [1773.8748191 ]\n",
      " [1717.09597376]\n",
      " [1751.87242313]\n",
      " [1828.81132244]\n",
      " [1625.07580682]\n",
      " [1784.59471255]\n",
      " [1886.26601376]\n",
      " [1919.73799688]\n",
      " [1933.68722241]\n",
      " [1645.50055376]\n",
      " [1549.95183381]\n",
      " [1623.43346756]\n",
      " [1824.91193458]\n",
      " [1823.85582546]\n",
      " [1698.81908328]\n",
      " [1773.59579632]\n",
      " [1642.43810699]\n",
      " [1933.40604493]\n",
      " [1996.31135203]\n",
      " [1763.60921507]\n",
      " [1723.43151161]\n",
      " [1999.95209436]\n",
      " [1817.8517993 ]\n",
      " [2141.99209395]\n",
      " [1608.48555295]\n",
      " [1802.16564431]\n",
      " [1945.50700988]\n",
      " [1660.81891313]\n",
      " [1981.75666725]\n",
      " [1942.96770497]\n",
      " [1802.16791541]\n",
      " [1874.68073025]\n",
      " [2144.8815976 ]\n",
      " [1667.64591475]\n",
      " [1861.51110556]\n",
      " [1969.97383259]\n",
      " [2029.28887037]\n",
      " [1621.696922  ]\n",
      " [2208.92358028]\n",
      " [1824.3749056 ]\n",
      " [1815.76690336]\n",
      " [1535.91968861]\n",
      " [3312.5507314 ]\n",
      " [1585.77958174]\n",
      " [1835.97162732]\n",
      " [1584.23696996]\n",
      " [1838.54981346]\n",
      " [1830.10896608]\n",
      " [1933.42995889]\n",
      " [1659.44774164]\n",
      " [1718.46796797]\n",
      " [1936.83714881]\n",
      " [1418.11008144]\n",
      " [2073.12870953]\n",
      " [1890.39051972]\n",
      " [1604.3233344 ]\n",
      " [1677.10300964]\n",
      " [1634.85713596]\n",
      " [1616.79445713]\n",
      " [1896.42635913]\n",
      " [1939.42733929]\n",
      " [1810.29361294]\n",
      " [1874.65116256]\n",
      " [1665.86762863]\n",
      " [1586.51209895]\n",
      " [1648.69842938]\n",
      " [1879.70707862]\n",
      " [1575.69751923]\n",
      " [1842.12832181]\n",
      " [1828.10678551]\n",
      " [1595.85511298]\n",
      " [1749.74642749]\n",
      " [1749.41011447]\n",
      " [1738.30601134]\n",
      " [1637.9190977 ]\n",
      " [1749.27562305]\n",
      " [1592.40100647]\n",
      " [1819.20184301]\n",
      " [1804.08351822]\n",
      " [1542.50374522]\n",
      " [2381.1458428 ]\n",
      " [1650.12743567]\n",
      " [1821.02110779]\n",
      " [1742.00634255]\n",
      " [1820.85069823]\n",
      " [1627.58105561]\n",
      " [1843.36217786]\n",
      " [1606.30350654]\n",
      " [1735.93504677]\n",
      " [1811.66930275]\n",
      " [1539.34840266]\n",
      " [1728.83647302]\n",
      " [1704.52539618]\n",
      " [1991.43570598]\n",
      " [1709.09745706]\n",
      " [1801.38936618]\n",
      " [1517.80116047]\n",
      " [1798.39870703]\n",
      " [1792.01920284]\n",
      " [1658.70898987]\n",
      " [1973.99103253]\n",
      " [1627.75997408]\n",
      " [1592.531633  ]\n",
      " [1707.8819024 ]\n",
      " [1494.84712979]\n",
      " [1658.85962007]\n",
      " [1586.91478484]\n",
      " [1933.60802908]\n",
      " [1686.70665164]\n",
      " [1978.53506771]\n",
      " [1994.29845686]\n",
      " [1769.42853852]\n",
      " [1772.47052229]\n",
      " [1795.92538702]\n",
      " [1958.51556144]\n",
      " [1912.44386201]\n",
      " [1825.39535736]\n",
      " [2003.69586973]\n",
      " [1579.03769569]\n",
      " [1592.76698176]\n",
      " [2000.69445796]\n",
      " [1761.9300788 ]\n",
      " [1929.63934301]\n",
      " [1554.25960251]\n",
      " [1720.15512672]\n",
      " [1595.84776507]\n",
      " [1815.48200254]\n",
      " [1847.53094457]\n",
      " [1977.00850968]\n",
      " [1904.28859457]\n",
      " [1819.94001071]\n",
      " [1874.4420406 ]\n",
      " [1953.90199718]\n",
      " [1711.13766187]\n",
      " [1885.23818457]\n",
      " [1875.92023594]\n",
      " [1930.29232718]\n",
      " [1958.98351541]\n",
      " [1735.54316716]\n",
      " [1947.52991303]\n",
      " [1865.76921339]\n",
      " [1811.66650873]\n",
      " [1963.8620012 ]\n",
      " [1991.3613231 ]\n",
      " [1570.60313715]\n",
      " [1865.65359692]\n",
      " [1659.8749504 ]\n",
      " [1679.96705598]\n",
      " [1907.73079473]\n",
      " [1727.41272747]\n",
      " [1943.48148053]\n",
      " [1823.39801194]\n",
      " [1926.7564644 ]\n",
      " [1944.75702412]\n",
      " [1893.09262399]\n",
      " [1485.38716428]\n",
      " [1922.62686489]\n",
      " [1878.7499362 ]\n",
      " [1606.13818915]\n",
      " [1837.84344047]\n",
      " [1599.26137663]\n",
      " [1953.01897856]\n",
      " [1590.30991019]\n",
      " [1956.67451734]\n",
      " [1768.20594233]\n",
      " [1958.01705068]\n",
      " [1799.14494376]\n",
      " [1668.49109931]\n",
      " [1977.10662914]\n",
      " [1705.74555801]\n",
      " [1575.7594771 ]\n",
      " [1808.96993259]\n",
      " [1818.53157443]\n",
      " [1865.77633764]\n",
      " [1727.66139951]\n",
      " [1924.50647522]\n",
      " [1579.02279878]\n",
      " [1780.64940081]\n",
      " [1853.2345628 ]\n",
      " [1945.58869275]\n",
      " [1856.92073011]\n",
      " [1842.55763621]\n",
      " [1863.56249954]\n",
      " [1914.8186261 ]\n",
      " [1975.82522545]\n",
      " [1648.45285692]\n",
      " [1981.37080856]\n",
      " [1921.55203442]\n",
      " [1522.20295353]\n",
      " [1797.93409279]\n",
      " [1675.83228095]\n",
      " [1771.49740765]\n",
      " [1651.08141083]\n",
      " [1567.79187298]\n",
      " [1639.02204532]\n",
      " [2072.06408035]\n",
      " [1747.92268801]\n",
      " [1941.95148572]\n",
      " [1776.89762459]\n",
      " [1625.79805393]\n",
      " [1659.71695141]\n",
      " [1603.59232588]\n",
      " [1918.60913666]\n",
      " [1772.89129865]\n",
      " [1742.20167946]\n",
      " [2058.38900552]\n",
      " [1865.87425586]\n",
      " [1716.44269692]\n",
      " [1956.13077282]\n",
      " [1592.50631519]\n",
      " [1655.39515753]\n",
      " [1535.23700275]\n",
      " [1617.35350332]\n",
      " [1782.38546917]\n",
      " [1957.96749405]\n",
      " [1892.25867675]\n",
      " [1562.86271851]\n",
      " [1838.29619852]\n",
      " [1632.82173934]\n",
      " [1598.63482157]\n",
      " [1874.40551346]\n",
      " [1534.06451098]\n",
      " [1952.13863737]\n",
      " [1997.19871517]\n",
      " [1647.11381882]\n",
      " [1642.89086238]\n",
      " [1886.15266931]\n",
      " [1726.15557836]\n",
      " [1593.13652682]\n",
      " [2143.22210837]\n",
      " [1672.01632424]\n",
      " [1488.42036813]\n",
      " [1870.81465305]\n",
      " [1893.9025155 ]\n",
      " [1663.28694455]\n",
      " [1677.24195272]\n",
      " [1971.56684332]\n",
      " [1870.3625784 ]\n",
      " [1764.77027338]\n",
      " [1774.66748983]\n",
      " [1591.25483653]\n",
      " [1737.47113264]\n",
      " [1638.94874123]\n",
      " [1664.19907301]\n",
      " [1480.27168649]\n",
      " [2062.43297458]\n",
      " [1917.35492942]\n",
      " [1641.48545371]\n",
      " [1819.94090213]\n",
      " [1866.4693929 ]\n",
      " [1894.04527957]\n",
      " [1594.27168136]\n",
      " [1696.27305804]\n",
      " [1700.99663398]\n",
      " [1853.01755174]\n",
      " [1829.31896036]\n",
      " [1564.74305287]\n",
      " [1666.65252808]\n",
      " [1590.24624689]\n",
      " [1965.73435872]\n",
      " [1913.36718284]\n",
      " [2024.0131198 ]\n",
      " [1896.20574069]\n",
      " [1770.5155922 ]\n",
      " [1680.93288388]\n",
      " [2466.08075966]\n",
      " [1857.35536141]\n",
      " [1768.08456492]\n",
      " [1639.29042744]\n",
      " [1640.39395336]\n",
      " [1959.95540113]\n",
      " [1781.53340549]\n",
      " [1979.20515973]\n",
      " [1590.14933438]\n",
      " [1710.42518461]\n",
      " [1747.82810194]\n",
      " [1990.6457441 ]\n",
      " [1853.49850269]\n",
      " [1994.65771536]\n",
      " [1838.21981617]\n",
      " [1778.84776235]\n",
      " [1717.65324337]\n",
      " [1578.89039162]\n",
      " [1723.50098628]\n",
      " [2160.04000428]\n",
      " [1953.53370116]\n",
      " [1702.3117258 ]\n",
      " [2101.80688528]\n",
      " [1674.30054193]\n",
      " [2589.26834183]\n",
      " [1964.08525693]\n",
      " [1757.24417295]\n",
      " [1569.94740363]\n",
      " [1749.88521327]\n",
      " [1680.7190266 ]\n",
      " [1625.43705097]\n",
      " [1717.20730326]\n",
      " [1585.71576485]\n",
      " [1593.07932901]\n",
      " [1639.79698707]\n",
      " [2470.00160404]\n",
      " [1822.65750845]\n",
      " [1727.98397304]\n",
      " [1736.02148843]\n",
      " [1733.38999075]\n",
      " [2164.52252989]\n",
      " [1816.47712964]\n",
      " [1746.93169632]\n",
      " [1799.6426197 ]\n",
      " [1343.14538544]\n",
      " [1672.60082156]\n",
      " [1575.14132497]\n",
      " [1930.11343716]\n",
      " [1521.5469937 ]\n",
      " [1795.72647   ]\n",
      " [1485.97594089]\n",
      " [1510.01483492]\n",
      " [1612.60917553]\n",
      " [1578.88810225]\n",
      " [1793.29065716]\n",
      " [1899.78250401]\n",
      " [1519.27138085]\n",
      " [1860.4694888 ]\n",
      " [1680.79901194]\n",
      " [1710.02989383]\n",
      " [1817.55792842]\n",
      " [1826.73070877]\n",
      " [1960.86169119]\n",
      " [1577.57742005]\n",
      " [1802.73559265]\n",
      " [1731.93394769]\n",
      " [1582.14914689]\n",
      " [1979.91033683]\n",
      " [1757.99029612]\n",
      " [1913.07063183]\n",
      " [1613.10053829]\n",
      " [1676.99618335]\n",
      " [1586.40650911]\n",
      " [1961.96167116]\n",
      " [1714.04403752]\n",
      " [1685.00670204]\n",
      " [1824.81188651]\n",
      " [2132.46354307]\n",
      " [1887.35256165]\n",
      " [1730.49771173]\n",
      " [1991.42023572]\n",
      " [1999.64462595]\n",
      " [1703.22262378]\n",
      " [1668.63371008]\n",
      " [1966.04536838]\n",
      " [1845.02267261]\n",
      " [1972.16031907]\n",
      " [1773.93052938]\n",
      " [1854.88897439]\n",
      " [2273.62856329]\n",
      " [1993.90380518]\n",
      " [1532.54648414]\n",
      " [1521.27852232]\n",
      " [1878.00651093]\n",
      " [1887.59699127]\n",
      " [1637.43395463]\n",
      " [1779.56237318]\n",
      " [1792.62627313]\n",
      " [1591.37289296]\n",
      " [1715.79090431]\n",
      " [1984.26960367]\n",
      " [1933.44392373]\n",
      " [1831.55121618]\n",
      " [1634.90503073]\n",
      " [2150.98484905]\n",
      " [1695.28566992]\n",
      " [1514.12176143]\n",
      " [1710.72533309]\n",
      " [1866.38125876]\n",
      " [1534.6464632 ]\n",
      " [1962.18525308]\n",
      " [1848.7158413 ]\n",
      " [2031.71949611]\n",
      " [1814.00192066]\n",
      " [1658.62871158]\n",
      " [2009.06026631]\n",
      " [1809.80149061]\n",
      " [1728.91745159]\n",
      " [1878.93848101]\n",
      " [1647.70762389]\n",
      " [1879.87145584]\n",
      " [1827.9285564 ]\n",
      " [1996.9592528 ]\n",
      " [1780.81312105]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "yy = y_pred-y_train\n",
    "\n",
    "print(\"{}\".format(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 0s 24us/sample - loss: 0.0951\n",
      "0.09514963994287465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1785fca44a8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3hU9b3v8fd3LplIEgi5oJTABitbkYsBgsXN8UqLqPVyjtpSbSvVFlttn/p46hbb09bu2taetlu351i7UXm01nptVboPKiKgu7WIgKAoKOBGDagk4RokkEl+549ZiUMySSbJTCZZ6/N6nnlmZq3fmvlmMXyy8pv1+y1zziEiIv4SynUBIiKSeQp3EREfUriLiPiQwl1ExIcU7iIiPhTJdQEAZWVlbvTo0bkuQ0RkQFmzZk2tc6481bp+Ee6jR49m9erVuS5DRGRAMbN3O1qnbhkRER9SuIuI+JDCXUTEh/pFn7uI+E9jYyPV1dU0NDTkupQBLz8/n4qKCqLRaNrbKNxFJCuqq6spKipi9OjRmFmuyxmwnHPU1dVRXV3NmDFj0t5O3TIikhUNDQ2UlpYq2HvJzCgtLe32X0AKdxHJGgV7ZvRkPw7ocH9l2y5++cwmNG2xiMiRBnS4r39/D3et2Mq+g/FclyIi/cyePXv47W9/26Ntzz33XPbs2ZPhivrWgA738qIYADX1h3JciYj0N52Fe1NTU6fbLl68mOLi4myU1WcGdLiXFSbCvVbhLiJtzJ8/n61bt1JZWckNN9zAihUrOPPMM7nsssuYOHEiABdddBFTp05l/PjxLFiwoHXb0aNHU1tby7Zt2xg3bhzf+MY3GD9+PLNmzeLgwYPt3mvu3Ll861vf4swzz+TYY4/lhRde4Morr2TcuHHMnTsXSPxCmTt3LhMmTGDixIncdtttAGzdupXZs2czdepUTj31VDZt2pSRn39AnwqpcBcZGH7ylzd4c8e+jL7miZ8azI/PH9/h+ltvvZUNGzawbt06AFasWMGqVavYsGFD6ymFCxcupKSkhIMHDzJt2jQuvvhiSktLj3idzZs389BDD3H33XfzhS98gT/96U98+ctfbvd+u3fvZtmyZSxatIjzzz+fv/3tb9xzzz1MmzaNdevW0dTUxPbt29mwYQNAa7fPvHnz+N3vfsfYsWN5+eWXueaaa1i2bFmv988AD/c8AOrqD+e4EhEZCE4++eQjzhW/4447eOKJJwB4//332bx5c7twHzNmDJWVlQBMnTqVbdu2pXzt888/HzNj4sSJHH300a1/HYwfP55t27Zx+umn88477/Cd73yH8847j1mzZlFfX89LL73EpZde2vo6hw5l5mB1QId78aA8QqYjd5H+rrMj7L5UUFDQ+njFihUsXbqUv//97wwaNIgzzjgj5bnksVis9XE4HE7ZLZPcLhQKHbFNKBQiHo8zdOhQ1q9fz7PPPsudd97Jo48+yu23305xcXHrXxeZNKD73MMho6QgpnAXkXaKiorYv39/h+v37t3L0KFDGTRoEJs2bWLlypVZrae2tpbm5mYuvvhifvrTn7J27VoGDx7MmDFjeOyxx4DEaNT169dn5P0GdLhDomumZr+6ZUTkSKWlpcyYMYMJEyZwww03tFs/e/Zs4vE4kyZN4oc//CHTp0/Paj3bt2/njDPOoLKykrlz5/KLX/wCgAcffJB7772Xk046ifHjx/PUU09l5P2sPwwAqqqqcj29WMdX7n2Z/Q1xnrx2RoarEpHe2LhxI+PGjct1Gb6Ran+a2RrnXFWq9gP+yL20II+6A+qWERFJNuDDvawwRq26ZUREjjDww70oxsHGJg4c0hQEIiItBn64ayCTiEg7Pgj3xECmWg1kEhFp5YNw15G7iEhbCncREU9hYWGuS8iYAR/upS3dMjpjRkSk1YAP92g4RPGgqI7cReQIN9544xHzud9888385je/ob6+npkzZzJlyhQmTpzY5YjQbdu2ccIJJ/D1r3+dCRMmcPnll7N06VJmzJjB2LFjWbVqFQAvvPAClZWVVFZWMnny5NapD371q18xbdo0Jk2axI9//OPs/cBtDOiJw1qUFcY0kEmkP3t6Pnz4emZf85iJcM6tHa6eM2cO1113Hddccw0Ajz76KM888wz5+fk88cQTDB48mNraWqZPn84FF1zQ6XVKt2zZwmOPPcaCBQuYNm0af/zjH/nrX//KokWL+PnPf86TTz7Jr3/9a+68805mzJhBfX09+fn5LFmyhM2bN7Nq1Sqcc1xwwQW8+OKLnHbaaZndFykM+CN3SIxSVbeMiCSbPHkyO3fuZMeOHaxfv56hQ4cyatQonHN8//vfZ9KkSXz2s59l+/btfPTRR52+1pgxY5g4cSKhUIjx48czc+bM1ul9W6YAnjFjBtdffz133HEHe/bsIRKJsGTJEpYsWcLkyZOZMmUKmzZtYvPmzX3w03fjyN3MwsBqYLtz7vNmNgZ4GCgB1gJfcc4dNrMY8HtgKlAHfNE5ty3jlScpK4qxMcMXAhCRDOrkCDubLrnkEh5//HE+/PBD5syZAyQm6qqpqWHNmjVEo1FGjx6dcqrfZG2n8E2e3jceTwygnD9/Pueddx6LFy9m+vTpLF26FOccN910E1dffXWWfsKOdefI/bvAxqTnvwRuc86NBXYDV3nLrwJ2O+eOA27z2mVVeWFM11EVkXbmzJnDww8/zOOPP84ll1wCJKb6HTZsGNFolOXLl/Puu+9m5L22bt3KxIkTufHGG6mqqmLTpk2cffbZLFy4kPr6eiAxM+TOnTsz8n5dSevI3cwqgPOAnwHXW6Jz6izgMq/J/cDNwF3Ahd5jgMeB/2tm5rI4/WRZYR77G+IcijcRi4Sz9TYiMsCMHz+e/fv3M2LECIYPHw7A5Zdfzvnnn09VVRWVlZWccMIJGXmv22+/neXLlxMOhznxxBM555xziMVibNy4kVNOOQVInGr5hz/8gWHDhmXkPTuT1pS/ZvY48AugCPgeMBdY6R2dY2YjgaedcxPMbAMw2zlX7a3bCnzGOVfb5jXnAfMARo0aNbU3vz0fWvUeN/35dV6afxafKj6qx68jIpmjKX8zK+NT/prZ54Gdzrk1yYtTNHVprPtkgXMLnHNVzrmq8vLyrsrolAYyiYgcKZ1umRnABWZ2LpAPDAZuB4rNLOKciwMVwA6vfTUwEqg2swgwBNiV8cqTfDK/jMJdRATSOHJ3zt3knKtwzo0G5gDLnHOXA8uBS7xmVwAtIwEWec/x1i/LZn87JB2563RIkX6lP1zpzQ96sh97c577jSS+XN0ClAL3esvvBUq95dcD83vxHmlpDXcNZBLpN/Lz86mrq1PA95Jzjrq6OvLz87u1XbdGqDrnVgArvMfvACenaNMAXNqtKnrpqLwwBXlhHbmL9CMVFRVUV1dTU1OT61IGvPz8fCoqKrq1jS+mH4DEQCb1uYv0H9FolDFjxuS6jMDyxfQD4F1LVeEuIgL4KtzzqNPVmEREAB+Fe6mO3EVEWvkm3MsKY+z6+DDxpuZclyIiknO+Cffywjycg10fq2tGRMQ34a6BTCIin/BPuBclwl1XZBIR8VG4lxZofhkRkRa+CfeWI3d1y4iI+Cjci2IR8iIhHbmLiOCjcDczygtj1Gogk4iIf8IdoLQwT0fuIiL4LNw1v4yISILPwl1H7iIi4Ltwj1FXf5jmZl0cQESCzXfhHm927GtozHUpIiI55atwL9WFskVEAJ+Fe7k3v0yNBjKJSMD5KtxbR6nqyF1EAs5f4V6ocBcRAZ+Fe/FRUcIh0+X2RCTwfBXuoZBRUqBz3UVEfBXuoFGqIiLgy3DPo0bdMiIScL4L9/LCGHU6cheRgPNduLfMDOmcpiAQkeDyXbiXFcZoaGzmwOGmXJciIpIzvgx3gNr96poRkeDyX7hrlKqIiA/DvXXyMJ0xIyLB5cNw15G7iIjvwr2kQNP+ioj4Ltyj4RBDB0UV7iISaL4Ld/jkcnsiIkHVZbibWb6ZrTKz9Wb2hpn9xFs+xsxeNrPNZvaImeV5y2Pe8y3e+tHZ/RHaK9WFskUk4NI5cj8EnOWcOwmoBGab2XTgl8BtzrmxwG7gKq/9VcBu59xxwG1euz6VmDxMR+4iElxdhrtLqPeeRr2bA84CHveW3w9c5D2+0HuOt36mmVnGKk5DWWFMg5hEJNDS6nM3s7CZrQN2As8BW4E9zrm416QaGOE9HgG8D+Ct3wuUpnjNeWa22sxW19TU9O6naKO8KMb+Q3EaGjUFgYgEU1rh7pxrcs5VAhXAycC4VM28+1RH6e1m8XLOLXDOVTnnqsrLy9OtNy0tA5nqDqhrRkSCqVtnyzjn9gArgOlAsZlFvFUVwA7vcTUwEsBbPwTYlYli01VaoPllRCTY0jlbptzMir3HRwGfBTYCy4FLvGZXAE95jxd5z/HWL3N9PP+u5pcRkaCLdN2E4cD9ZhYm8cvgUefcf5jZm8DDZnYL8Cpwr9f+XuABM9tC4oh9Thbq7tQn88so3EUkmLoMd+fca8DkFMvfIdH/3nZ5A3BpRqrroU/ml1Gfu4gEky9HqOZHwxTGIjpyF5HA8mW4Q6JrRkfuIhJUPg53DWQSkeDyd7irW0ZEAsq/4V6Up0FMIhJYvg330oIYuz8+TLypOdeliIj0Od+Ge1lRDOdgl47eRSSAfBvu5d5Aphr1u4tIAPk23FsGMumKTCISRL4N99JCzS8jIsHl23DX/DIiEmS+DffCWIRYJKRRqiISSL4NdzPTKFURCSzfhjskToes1amQIhJA/g73gjwduYtIIPk73DW/jIgElL/D3Ztfprm5T6/yJyKSc/4O98IYTc2OPQcbc12KiEif8n24A9Spa0ZEAsbX4V6q+WVEJKB8He7lulC2iASUr8O9pVtGp0OKSND4OtyHHBUlEjLqDijcRSRYfB3uoZBRUpBH7X51y4hIsPg63EEDmUQkmPwf7kUKdxEJHv+He2GezpYRkcDxfbiXe90yzmkKAhEJDt+He2lhHofizdQfiue6FBGRPuP7cC/TQCYRCaAAhbu+VBWR4AhMuGvyMBEJkgCEe8vkYeqWEZHg8H24lxTkYab5ZUQkWLoMdzMbaWbLzWyjmb1hZt/1lpeY2XNmttm7H+otNzO7w8y2mNlrZjYl2z9EZyLhEEMH5anPXUQCJZ0j9zjwP51z44DpwLVmdiIwH3jeOTcWeN57DnAOMNa7zQPuynjV3ZQYyKRwF5Hg6DLcnXMfOOfWeo/3AxuBEcCFwP1es/uBi7zHFwK/dwkrgWIzG57xyruhrDBGnfrcRSRAutXnbmajgcnAy8DRzrkPIPELABjmNRsBvJ+0WbW3rO1rzTOz1Wa2uqampvuVd0OpJg8TkYBJO9zNrBD4E3Cdc25fZ01TLGs39t85t8A5V+WcqyovL0+3jB7R/DIiEjRphbuZRUkE+4POuT97iz9q6W7x7nd6y6uBkUmbVwA7MlNuz5QVxqg/FKehsSmXZYiI9Jl0zpYx4F5go3PuX5NWLQKu8B5fATyVtPyr3lkz04G9Ld03uVKuUaoiEjCRNNrMAL4CvG5m67xl3wduBR41s6uA94BLvXWLgXOBLcDHwNcyWnEPlHoDmWrrD1MxdFCOqxERyb4uw90591dS96MDzEzR3gHX9rKujNKFskUkaHw/QhUSV2MCdcuISHAEItxLC1q6ZRTuIhIMgQj3/GiYovyITocUkcAIRLhDot9dR+4iEhQBCnfNLyMiwRGgcI+pW0ZEAiNg4a4jdxEJhkCF+56PG2lsas51KSIiWReYcG8ZpbrrgLpmRMT/AhPuLaNUazRKVUQCIDDhXl6kgUwiEhyBCfeWI3ddkUlEgiAw4V6qaX9FJEACE+4FeWHyoyGFu4gEQmDC3cw0kElEAiMw4Q4ayCQiwRHAcNeRu4j4X8DCXZOHiUgwBCzcY+w6cJjmZpfrUkREsipg4Z5HU7Nj98fqmhERfwtWuHvXUq3T/DIi4nOBCvfSAm8gk+aXERGfC1S4t8wvU6MvVUXE5wIV7mWtUxCoW0ZE/C1Q4T7kqCjRsOl0SBHxvUCFu5lRWhCjTuEuIj4XqHCHxBWZ1C0jIn4XuHDX/DIiEgTBDHedCikiPhe8cC/Ko/bAYZzTFAQi4l/BC/eCGIfjzew/FM91KSIiWRO8cG+5ULa6ZkTEx4IX7hrIJCIBEOBw15G7iPhXYMNdA5lExM+6DHczW2hmO81sQ9KyEjN7zsw2e/dDveVmZneY2RYze83MpmSz+J4YOiiKGdSoW0ZEfCydI/f7gNltls0HnnfOjQWe954DnAOM9W7zgLsyU2bmRMIhSgbpcnsi4m9dhrtz7kVgV5vFFwL3e4/vBy5KWv57l7ASKDaz4ZkqNlM0kElE/K6nfe5HO+c+APDuh3nLRwDvJ7Wr9pa1Y2bzzGy1ma2uqanpYRk9U1aUp6sxiYivZfoLVUuxLOVQUOfcAudclXOuqry8PMNldE7zy4iI3/U03D9q6W7x7nd6y6uBkUntKoAdPS8vO0oL1C0jIv7W03BfBFzhPb4CeCpp+Ve9s2amA3tbum/6k7KiPA4cbuLg4aZclyIikhXpnAr5EPB34Hgzqzazq4Bbgc+Z2Wbgc95zgMXAO8AW4G7gmqxU3UsayCQifhfpqoFz7ksdrJqZoq0Dru1tUdlWnhTuI0sG5bgaEZHMC9wIVUhcjQk0v4yI+Fcgw13dMiLidwM73KtXw//7HnTzwhutR+46Y0ZEfGpgh/uHr8Mrd8PGRd3aLBYJMzg/oiN3EfGtgR3uU74Kw8bDcz+CePeCuqwoRq1GqYqITw3scA+F4exbYPc2ePnfu7VpmQYyiYiPDexwB/j0WTB2Frz4KzhQm/ZmZUWaGVJE/GvghzvArFvg8AFYcWvXbT2J+WXULSMi/uSPcC8/HqquhNULoeattDYpK4yx92Ajh+PNWS5ORKTv+SPcAc6YD3mFsOR/pdW85XTIXfpSVUR8yD/hXlAGp30PNi+BLc932VwDmUTEz/wT7gCfuRqGjk4cvTd3PuNjS7jXKNxFxIf8Fe6RGHzuX2Dnm/DqA502bZ08TKdDiogP+SvcAcZdAKNOgWW3QMO+DpuVFSX63HW5PRHxI/+Fuxmc/TM4UAN/va3DZoPyIhwVDevIXUR8yX/hDjBiKkz6Ivz9TtjzXofNNJBJRPzKn+EOMPNHYCFY+pMOm2ggk4j4lX/DfUgF/NN3YMPj8P4rKZskwl1H7iLiP/4Nd4AZ34XCo+HZm1LO+V5WmKcjdxHxJX+He6wQzvohVL8Cb/y53eqywhi7DhyisUlTEIiIv/g73AEqL4NjJsJzN0NjwxGrxn9qCM0Orn5gDfWH4rmpT0QkC/wf7qEwzPoZ7H0PXr7riFWzJxzDLRdN4IW3a7jkrpeo3v1xjooUEcks/4c7wLGnw/Hnwou/gfqdR6z68vR/4L6vTWP7noNcdOdLvPre7hwVKSKSOcEId4DP/RTiB2H5z9utOnVsOU9c808MygvzxQUr+cv6HTkoUEQkc4IT7mXHwbRvwNr74aM3260+blgRT147g8qKYr7z0Kv829LNuBRn2IiIDATBCXeA0/8ZYoM7nPO9pCCPB75+MhdPqeC2pW/z3YfX0dDY+eySIiL9UbDCfVAJnH4jbH0eNj+XskksEubXl07in2cfz6L1O7js7pXUaP4ZERlgghXuANO+DiWfhmd/AE2pT380M6454zjuunwKb36wj4vu/Btvfbi/jwsVEem54IV7JA9m/RRq34K193Xa9JyJw3n06lNobGrm4rteYvlbOzttLyLSXwQv3CFxWuToUxNnzjTs7bTppIpinvr2DEaVDOKq+17hvr/9Vx8VKSLSc8EMdzOYdQt8vAte/HWXzYcPOYrHvnkKZ51wNDf/5U1++OQG4pqyQET6sWCGO8CnKhNTE7x0B/zOO4rf8WrKCcYACmIR/v0rU7n6tGN5YOW7fO2+V9h7sLGPixYRSY/1h3O5q6qq3OrVq/v+jQ/Vwyv3wFtPQ/UqcM1QNBz+8exE182Y0yB6VLvNHnnlPX7wxAZGlxWw8IppjCod1Pe1i0jgmdka51xVynWBDvdkB+pg8xJ4azFsXQaH6yE6CI49A44/B8aeDUVHtzZ/aWst3/rDWhqbmpkyaignjRzCSRXFVI4sZtjg/Jz9GCISHAr37oofgm3/CW89A28/A3vfTywfMTUR9P94Dhw9nv+q+5i7//Md1r+/h00f7qepObEvhw/J56SKYk4aWcxJI4cwccQQivKjOfyBRMSP+jzczWw28G9AGLjHOXdrZ+37Xbgncw4+eiPRdfP207B9TWL5kFFw/OzEkX1BOQ3hQt7aG+LVnc2s3d7A+u17ebcuMcukGRxXXuiFfTGVFcUcf0wReZHgfuUhIr3Xp+FuZmHgbeBzQDXwCvAl51z7CV08/Trc29r/Ibz9bOKIfuvyxGRkbYWikD+YprzBHAgVsqfpKHY2xqg+GKWmMZ/9bhAHQgUMLi6jpKSEaDRKOBzFIlEi4TChcIRwJEo4EiEcjhKJRIhEooQjUSItt2iUaCRCJBohFIoQCocS96Fw4nE4TDgUxkIRwuEw4VCIUAhCZoRDlnSfGLQlIgNPZ+EeycL7nQxscc694735w8CFQIfhPqAUHQNTr0jcGg/ChxugYU/ifPmGvXBoX+vjcMM+BjfsZfChfYxqqGFqeC+uYS+hRm/e+P3erY/EXYhmDEeIOCEOYzRjgOGAZkIkftUbrvX2yfPWtgYuaTs48pdDy9K2kpdbiu1atN3atfnl4zrYrqP1yc+tk+2PrLttrW2fd/TeHb9i8pZd/QwdvUJ31/cF18XPlfzv59rtzfY62jM9/VnT3dddvU+qV+mwphSLO6p+59Trmfr5b6RVW3dkI9xHAO8nPa8GPtO2kZnNA+YBjBo1Kgtl9IHoUTByWtrNzbvR1AiH9id+KRyqh+Y4NDfhmuM0xhtpisdpbGykKd5IPB6nqcm7jyeWNTXFaYrHaW5K3FxzM7gmXHMTuGZcczPONUHSc5x3a27CeY/NNSXW4bxTQB3mmhMfwtbn7oj1iddxXkg2Y+6TD+2R0WifLDmiTZvgbfOJb/+fpaN4bNHcfv0Rm3Txes6lDJ9UugqxI395dRxER7Rr8/6dvm83AiMXkg8FPpH02KVan1qH/w7d/Suzi54Jw6X+hdTJr5gj27l2yzpo2tEiYoPLO62xp7IR7qnqb7eHnXMLgAWQ6JbJQh39VziamMRsUMkRiw3I8x63PwFTRCR92fhGrxoYmfS8AtDVL0RE+lA2wv0VYKyZjTGzPGAOsCgL7yMiIh3IeLeMcy5uZt8GniVxKuRC59wbmX4fERHpWDb63HHOLQYWZ+O1RUSkaxpFIyLiQwp3EREfUriLiPiQwl1ExIf6xayQZlYDvNvDzcuA2gyWk2mqr3dUX+/19xpVX8/9g3Mu5RDXfhHuvWFmqzuaOKc/UH29o/p6r7/XqPqyQ90yIiI+pHAXEfEhP4T7glwX0AXV1zuqr/f6e42qLwsGfJ+7iIi054cjdxERaUPhLiLiQwMm3M1stpm9ZWZbzGx+ivUxM3vEW/+ymY3uw9pGmtlyM9toZm+Y2XdTtDnDzPaa2Trv9qO+qs97/21m9rr33u0uWGsJd3j77zUzm9KHtR2ftF/Wmdk+M7uuTZs+339mttDMdprZhqRlJWb2nJlt9u6HdrDtFV6bzWZ2RR/V9isz2+T9+z1hZsUdbNvpZyHLNd5sZtuT/h3P7WDbTv+/Z7G+R5Jq22Zm6zrYtk/2Ya845/r9jcTUwVuBY0lcrGg9cGKbNtcAv/MezwEe6cP6hgNTvMdFJC4Q3ra+M4D/yOE+3AaUdbL+XOBpEheEmg68nMN/6w9JDM7I6f4DTgOmABuSlv1vYL73eD7wyxTblQDvePdDvcdD+6C2WUDEe/zLVLWl81nIco03A99L4zPQ6f/3bNXXZv1vgB/lch/25jZQjtxbL7rtnDsMtFx0O9mFwP3e48eBmWbdveBizzjnPnDOrfUe7wc2kriW7EByIfB7l7ASKDaz4TmoYyaw1TnX0xHLGeOcexHY1WZx8ufsfuCiFJueDTznnNvlnNsNPAfMznZtzrklzrm493Qliaug5UwH+y8d6fx/77XO6vOy4wvAQ5l+374yUMI91UW324ZnaxvvA74XKO2T6pJ43UGTgZdTrD7FzNab2dNmNr5PC0tcx3aJma3xLk7eVjr7uC/MoeP/ULncfy2Ods59AIlf6sCwFG36w768ksRfYql09VnItm97XUcLO+jW6g/771TgI+fc5g7W53ofdmmghHs6F91O68Lc2WRmhcCfgOucc/varF5LoqvhJOD/AE/2ZW3ADOfcFOAc4FozO63N+v6w//KAC4DHUqzO9f7rjpzuSzP7ARAHHuygSVefhWy6C/g0UAl8QKLro62cfxaBL9H5UXsu92FaBkq4p3PR7dY2ZhYBhtCzPwl7xMyiJIL9Qefcn9uud87tc87Ve48XA1EzK+ur+pxzO7z7ncATJP70TdYfLmx+DrDWOfdR2xW53n9JPmrprvLud6Zok7N96X15+3ngcud1DreVxmcha5xzHznnmpxzzcDdHbx3Tj+LXn78D+CRjtrkch+ma6CEezoX3V4EtJyVcAmwrKMPd6Z5/XP3Ahudc//aQZtjWr4DMLOTSez7uj6qr8DMiloek/jibUObZouAr3pnzUwH9rZ0P/ShDo+Wcrn/2kj+nF0BPJWizbPALDMb6nU7zPKWZZWZzQZuBC5wzn3cQZt0PgvZrDH5e5z/3sF7p/P/PZs+C2xyzlWnWpnrfZi2XH+jm+6NxNkcb5P4Fv0H3rJ/IfFBBsgn8ef8FmAVcGwf1vbfSPzZ+BqwzrudC3wT+KbX5tvAGyS++V8J/FMf1nes977rvRpa9l9yfQbc6e3f14GqPv73HUQirIckLcvp/iPxi+YDoJHE0eRVJL7HeR7Y7N2XeG2rgHuStr3S+yxuAb7WR7VtIdFX3fIZbDl77FPA4s4+C324/x7wPl+vkQjs4W1r9J63+//eF/V5y+9r+dwltc3JPuzNTdMPiIj40EDplhERkXiD1CMAAAAoSURBVG5QuIuI+JDCXUTEhxTuIiI+pHAXEfEhhbuIiA8p3EVEfOj/A0sHyxCt52ZsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model.evaluate(X_test_transformed, y_test))\n",
    "\n",
    "train_acc = history.history['loss']\n",
    "val_acc = history.history['val_loss']\n",
    "x = np.arange(len(train_acc))\n",
    "plt.plot(x, train_acc, label = 'train mse')\n",
    "plt.plot(x, val_acc, label = 'val mse')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T11:16:09.465072Z",
     "start_time": "2019-10-30T11:16:08.747951Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train  = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:,  np.newaxis])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T11:16:29.211993Z",
     "start_time": "2019-10-30T11:16:28.946704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ykenk\\Anaconda3\\envs\\tf_keras\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(200, input_dim=784, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(200, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer = tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T11:16:46.435476Z",
     "start_time": "2019-10-30T11:16:40.397525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 1.7702 - acc: 0.4114\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.7155 - acc: 0.7657\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.4462 - acc: 0.8637\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3366 - acc: 0.8994\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2742 - acc: 0.9182\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2307 - acc: 0.9305\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1953 - acc: 0.9422\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.1714 - acc: 0.9491\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.1505 - acc: 0.9548\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1329 - acc: 0.9596\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   batch_size=12000,\n",
    "                   epochs=10,\n",
    "                   verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
